name: NextDNS Unified Workflow

# Este workflow consolidado substitui m√∫ltiplos workflows duplicados
# Executa captura de logs, deploy Hugo e integra√ß√µes com outros servi√ßos

on:
  schedule:
    - cron: '30 1 * * *' # Uma vez por dia √†s 01:30 UTC
  workflow_dispatch:
    inputs:
      skip_integrations:
        description: 'Skip external integrations (Datadog, Axiom)'
        required: false
        default: false
        type: boolean
      force_rebuild:
        description: 'Force rebuild of Hugo site'
        required: false  
        default: false
        type: boolean

permissions:
  contents: write
  pages: write
  id-token: write

env:
  NEXTDNS_PROFILE_ID: '85d564'
  HUGO_VERSION: '0.121.0'

jobs:
  nextdns-pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    
    outputs:
      logs-fetched: ${{ steps.fetch-logs.outputs.success }}
      hugo-built: ${{ steps.build-hugo.outputs.success }}
      
    steps:
      # ================================================
      # SETUP AND PREPARATION
      # ================================================
      
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          token: ${{ secrets.GITHUB_TOKEN }}

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'

      - name: Setup Hugo
        uses: peaceiris/actions-hugo@v2
        with:
          hugo-version: ${{ env.HUGO_VERSION }}
          extended: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install requests jinja2 python-dateutil

      # ================================================
      # FETCH NEXTDNS LOGS
      # ================================================
      
      - name: Fetch NextDNS Logs
        id: fetch-logs
        env:
          NEXTDNS_API_KEY: ${{ secrets.NEXTDNS_API_KEY }}
        run: |
          mkdir -p data/logs
          DATE=$(date +'%Y-%m-%d')
          TIMESTAMP=$(date +'%Y-%m-%d_%H-%M-%S')
          
          echo "üîÑ Fetching NextDNS logs..."
          
          # Fetch logs with retry mechanism
          for attempt in 1 2 3; do
            if curl -s -f \
              -H "X-Api-Key: ${NEXTDNS_API_KEY}" \
              -H "Accept: application/json" \
              "https://api.nextdns.io/profiles/${NEXTDNS_PROFILE_ID}/logs" \
              -o "data/logs/raw_${TIMESTAMP}.json"; then
              echo "‚úÖ Logs fetched successfully"
              break
            else
              echo "‚ùå Attempt $attempt failed"
              if [ $attempt -eq 3 ]; then
                echo "‚ùå Failed to fetch logs after 3 attempts"
                echo "success=false" >> $GITHUB_OUTPUT
                exit 0  # Don't fail the whole workflow
              fi
              sleep 10
            fi
          done
          
          echo "success=true" >> $GITHUB_OUTPUT

      - name: Process and Analyze Logs  
        if: steps.fetch-logs.outputs.success == 'true'
        run: |
          python3 << 'EOF'
          import json
          import os
          from datetime import datetime, timedelta
          from collections import Counter, defaultdict
          
          # Load the raw logs
          timestamp = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')
          raw_file = f"data/logs/raw_{timestamp}.json"
          
          # Find the most recent raw log file
          import glob
          raw_files = glob.glob("data/logs/raw_*.json")
          if not raw_files:
              print("‚ùå No raw log files found")
              exit(1)
              
          latest_raw = max(raw_files)
          print(f"üìñ Processing {latest_raw}")
          
          with open(latest_raw, 'r') as f:
              logs_data = json.load(f)
          
          if 'data' not in logs_data:
              print("‚ùå Invalid log structure")
              exit(1)
              
          logs = logs_data['data']
          print(f"üìä Processing {len(logs)} log entries")
          
          # Create processed data structure
          processed_data = {
              'metadata': {
                  'timestamp': datetime.now().isoformat(),
                  'total_entries': len(logs),
                  'profile_id': os.environ.get('NEXTDNS_PROFILE_ID'),
                  'date_range': {
                      'start': logs[-1]['timestamp'] if logs else None,
                      'end': logs[0]['timestamp'] if logs else None
                  }
              },
              'statistics': {},
              'top_domains': {},
              'recent_logs': logs[:50]  # Last 50 for display
          }
          
          # Calculate statistics
          status_count = Counter(log.get('status', 'unknown') for log in logs)
          type_count = Counter(log.get('type', 'unknown') for log in logs)
          domain_count = Counter(log.get('name', 'unknown') for log in logs)
          client_count = Counter(log.get('clientIp', 'unknown') for log in logs)
          
          processed_data['statistics'] = {
              'by_status': dict(status_count),
              'by_type': dict(type_count), 
              'by_client': dict(client_count.most_common(10)),
              'blocked_percentage': round((status_count.get('blocked', 0) / len(logs)) * 100, 2) if logs else 0
          }
          
          processed_data['top_domains'] = {
              'all': dict(domain_count.most_common(20)),
              'blocked': dict(Counter(
                  log.get('name') for log in logs 
                  if log.get('status') == 'blocked'
              ).most_common(10))
          }
          
          # Save processed data
          os.makedirs('content/data', exist_ok=True)
          with open('content/data/processed_logs.json', 'w') as f:
              json.dump(processed_data, f, indent=2)
          
          # Create summary for Hugo
          with open('content/data/summary.json', 'w') as f:
              json.dump({
                  'last_update': datetime.now().isoformat(),
                  'total_queries': len(logs),
                  'blocked_queries': status_count.get('blocked', 0),
                  'allowed_queries': status_count.get('allowed', 0),
                  'blocked_percentage': processed_data['statistics']['blocked_percentage']
              }, f, indent=2)
              
          print("‚úÖ Log processing completed")
          EOF

      # ================================================
      # BUILD HUGO SITE
      # ================================================
      
      - name: Prepare Hugo Site
        if: steps.fetch-logs.outputs.success == 'true' || github.event.inputs.force_rebuild == 'true'
        run: |
          echo "üèóÔ∏è Preparing Hugo site structure..."
          
          # Create basic Hugo structure if it doesn't exist
          if [ ! -f "config.toml" ]; then
            cat << 'EOF' > config.toml
          baseURL = "https://diogo0587.github.io/lognextdns"
          languageCode = "pt-br"
          title = "NextDNS Logs Dashboard"
          theme = "ananke"
          
          [params]
            description = "Dashboard de logs do NextDNS"
            github_url = "https://github.com/diogo0587/lognextdns"
          
          [markup]
            [markup.goldmark]
              [markup.goldmark.renderer]
                unsafe = true
          EOF
          fi
          
          # Setup theme
          if [ ! -d "themes/ananke" ]; then
            echo "üì• Installing Hugo theme..."
            git submodule add https://github.com/theNewDynamic/gohugo-theme-ananke.git themes/ananke || {
              # If submodule fails, clone directly
              git clone https://github.com/theNewDynamic/gohugo-theme-ananke.git themes/ananke
            }
          fi
          
          # Create content structure
          mkdir -p content/{posts,pages} layouts/{_default,partials} static/{css,js}
          
          # Create main page
          cat << 'EOF' > content/_index.md
          ---
          title: "NextDNS Logs Dashboard"
          description: "Dashboard de monitoramento dos logs do NextDNS"
          ---
          
          # Dashboard NextDNS
          
          Este dashboard apresenta informa√ß√µes em tempo real dos logs do NextDNS.
          
          ## Estat√≠sticas Recentes
          
          Os dados s√£o atualizados automaticamente via GitHub Actions.
          EOF

      - name: Generate Hugo Content from Logs
        if: steps.fetch-logs.outputs.success == 'true'
        run: |
          python3 << 'EOF'
          import json
          import os
          from datetime import datetime
          
          # Load processed data
          try:
              with open('content/data/processed_logs.json', 'r') as f:
                  data = json.load(f)
          except FileNotFoundError:
              print("‚ùå No processed data found")
              exit(1)
          
          stats = data['statistics']
          domains = data['top_domains']
          
          # Create statistics page
          stats_content = f"""---
          title: "Estat√≠sticas de Logs"
          date: {datetime.now().isoformat()}
          draft: false
          ---
          
          # Estat√≠sticas NextDNS
          
          **√öltima atualiza√ß√£o:** {data['metadata']['timestamp']}
          
          ## Resumo Geral
          
          - **Total de consultas:** {data['metadata']['total_entries']:,}
          - **Consultas bloqueadas:** {stats['by_status'].get('blocked', 0):,}
          - **Consultas permitidas:** {stats['by_status'].get('allowed', 0):,}
          - **Taxa de bloqueio:** {stats['blocked_percentage']}%
          
          ## Top 10 Dom√≠nios Mais Acessados
          
          """
          
          for i, (domain, count) in enumerate(domains['all'].items(), 1):
              if i <= 10:
                  stats_content += f"{i}. **{domain}** - {count:,} consultas\n"
          
          stats_content += "\n## Top 10 Dom√≠nios Bloqueados\n\n"
          
          for i, (domain, count) in enumerate(domains['blocked'].items(), 1):
              if i <= 10:
                  stats_content += f"{i}. **{domain}** - {count:,} bloqueios\n"
          
          with open('content/statistics.md', 'w') as f:
              f.write(stats_content)
          
          print("‚úÖ Hugo content generated")
          EOF

      - name: Build Hugo Site
        id: build-hugo
        if: steps.fetch-logs.outputs.success == 'true' || github.event.inputs.force_rebuild == 'true'
        run: |
          echo "üî® Building Hugo site..."
          
          # Build the site
          if hugo --minify --cleanDestinationDir; then
            echo "‚úÖ Hugo build successful"
            echo "success=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Hugo build failed"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Verify output
          if [ -d "public" ] && [ "$(ls -A public)" ]; then
            echo "‚úÖ Public directory contains files"
            ls -la public/
          else
            echo "‚ùå Public directory is empty or missing"
            exit 1
          fi

      # ================================================
      # EXTERNAL INTEGRATIONS
      # ================================================
      
      - name: Send to Datadog (Optional)
        if: |
          steps.fetch-logs.outputs.success == 'true' && 
          github.event.inputs.skip_integrations != 'true' &&
          secrets.DATADOG_API_KEY != ''
        continue-on-error: true
        env:
          DATADOG_API_KEY: ${{ secrets.DATADOG_API_KEY }}
        run: |
          if [ -f "content/data/summary.json" ]; then
            python3 << 'EOF'
          import json
          import requests
          import os
          from datetime import datetime
          
          # Load summary data
          with open('content/data/summary.json', 'r') as f:
              data = json.load(f)
          
          # Send metrics to Datadog
          datadog_api_key = os.environ.get('DATADOG_API_KEY')
          if datadog_api_key:
              payload = {
                  "series": [
                      {
                          "metric": "nextdns.total_queries",
                          "points": [[int(datetime.now().timestamp()), data['total_queries']]],
                          "tags": ["source:nextdns", "profile:85d564"]
                      },
                      {
                          "metric": "nextdns.blocked_queries", 
                          "points": [[int(datetime.now().timestamp()), data['blocked_queries']]],
                          "tags": ["source:nextdns", "profile:85d564"]
                      }
                  ]
              }
              
              try:
                  response = requests.post(
                      "https://api.datadoghq.com/api/v1/series",
                      headers={"DD-API-KEY": datadog_api_key, "Content-Type": "application/json"},
                      json=payload,
                      timeout=30
                  )
                  response.raise_for_status()
                  print("‚úÖ Metrics sent to Datadog")
              except Exception as e:
                  print(f"‚ùå Failed to send to Datadog: {e}")
          EOF
          fi

      # ================================================
      # COMMIT CHANGES AND DEPLOY
      # ================================================
      
      - name: Commit Changes
        if: steps.fetch-logs.outputs.success == 'true'
        run: |
          git config --local user.name "github-actions[bot]"
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          
          git add .
          
          if git diff --staged --quiet; then
            echo "‚ÑπÔ∏è  No changes to commit"
          else
            git commit -m "üìä Update NextDNS logs and dashboard - $(date +'%Y-%m-%d %H:%M')" \
                       -m "- Fetched and processed latest NextDNS logs" \
                       -m "- Updated Hugo dashboard" \
                       -m "- Automated via unified workflow"
            git push
            echo "‚úÖ Changes committed and pushed"
          fi

      - name: Deploy to GitHub Pages
        if: steps.build-hugo.outputs.success == 'true'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./public
          force_orphan: true
          commit_message: "üöÄ Deploy NextDNS dashboard - $(date +'%Y-%m-%d %H:%M')"

      # ================================================
      # CLEANUP AND NOTIFICATIONS
      # ================================================
      
      - name: Cleanup Old Files
        run: |
          # Remove old raw log files (keep last 5)
          find data/logs -name "raw_*.json" -type f | sort -r | tail -n +6 | xargs rm -f || true
          
          echo "üßπ Cleanup completed"

      - name: Workflow Summary
        if: always()
        run: |
          echo "## üìä NextDNS Unified Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "**Logs Fetched:** ${{ steps.fetch-logs.outputs.success }}" >> $GITHUB_STEP_SUMMARY  
          echo "**Hugo Built:** ${{ steps.build-hugo.outputs.success }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ steps.fetch-logs.outputs.success }}" == "true" ]; then
            if [ -f "content/data/summary.json" ]; then
              echo "### üìà Statistics" >> $GITHUB_STEP_SUMMARY
              python3 -c "
          import json
          with open('content/data/summary.json', 'r') as f:
              data = json.load(f)
          print(f'- Total Queries: {data[\"total_queries\"]:,}')
          print(f'- Blocked: {data[\"blocked_queries\"]:,}')  
          print(f'- Block Rate: {data[\"blocked_percentage\"]}%')
          " >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "‚ùå Log fetching failed - check NextDNS API key and connectivity" >> $GITHUB_STEP_SUMMARY
          fi