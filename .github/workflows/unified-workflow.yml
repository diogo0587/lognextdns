name: NextDNS Unified Workflow

on:
  schedule:
    - cron: '30 1 * * *'
  workflow_dispatch:
    inputs:
      skip_integrations:
        description: 'Skip external integrations (Datadog, Axiom)'
        required: false
        default: false
        type: boolean
      force_rebuild:
        description: 'Force rebuild of Hugo site'
        required: false
        default: false
        type: boolean

permissions:
  contents: write
  pages: write
  id-token: write

env:
  NEXTDNS_PROFILE_ID: ${{ secrets.NEXTDNS_PROFILE_ID }}
  HUGO_VERSION: '0.124.1'

jobs:
  nextdns-pipeline:
    runs-on: ubuntu-latest
    timeout-minutes: 20
    outputs:
      fetch_logs_success: ${{ steps.fetch-logs.outputs.success }}
      build_hugo_success: ${{ steps.build-hugo.outputs.success }}
    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Setup Hugo
        uses: peaceiris/actions-hugo@v2
        with:
          hugo-version: ${{ env.HUGO_VERSION }}
          extended: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install requests jinja2 python-dateutil

      - name: Fetch NextDNS Logs
        id: fetch-logs
        env:
          NEXTDNS_API_KEY: ${{ secrets.NEXTDNS_API_KEY }}
        run: |
          mkdir -p data/logs
          TIMESTAMP=$(date +'%Y-%m-%d_%H-%M-%S')
          echo "üîÑ Fetching NextDNS logs..."
          for attempt in 1 2 3; do
            if curl -s -f \
              -H "X-Api-Key: ${NEXTDNS_API_KEY}" \
              -H "Accept: application/json" \
              "https://api.nextdns.io/profiles/${NEXTDNS_PROFILE_ID}/logs" \
              -o "data/logs/raw_${TIMESTAMP}.json"; then
              echo "‚úÖ Logs fetched successfully"
              echo "success=true" >> $GITHUB_OUTPUT
              break
            else
              echo "‚ùå Attempt $attempt failed"
              if [ $attempt -eq 3 ]; then
                echo "‚ùå Failed to fetch logs after 3 attempts"
                echo "success=false" >> $GITHUB_OUTPUT
                exit 1
              fi
              sleep 10
            fi
          done
          if [ ! -v GITHUB_OUTPUT ]; then
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Process and Analyze Logs
        id: process-logs
        if: steps.fetch-logs.outputs.success == 'true'
        run: |
          python3 << 'EOF'
          import json
          import os
          from datetime import datetime
          from collections import Counter
          import glob

          raw_files = glob.glob("data/logs/raw_*.json")
          if not raw_files:
              print("‚ùå No raw log files found")
              exit(1)
          latest_raw = max(raw_files)
          print(f"üìñ Processing {latest_raw}")
          with open(latest_raw, 'r') as f:
              try:
                  logs_data = json.load(f)
              except json.JSONDecodeError as e:
                  print(f"‚ùå Invalid JSON in log file: {e}")
                  exit(1)
          if 'data' not in logs_data:
              print("‚ùå Invalid log structure")
              exit(1)
          logs = logs_data['data']
          print(f"üìä Processing {len(logs)} log entries")
          processed_data = {
              'metadata': {
                  'timestamp': datetime.now().isoformat(),
                  'total_entries': len(logs),
                  'profile_id': os.environ.get('NEXTDNS_PROFILE_ID'),
                  'date_range': {
                      'start': logs[-1]['timestamp'] if logs else None,
                      'end': logs[0]['timestamp'] if logs else None
                  }
              },
              'statistics': {},
              'top_domains': {},
              'recent_logs': logs[:50]
          }
          status_count = Counter(status.get('status', 'unknown') if isinstance(status := log, dict) else 'unknown' for log in logs)
          # The above line attempted a safe pattern; fallback to original counters:
          status_count = Counter(log.get('status', 'unknown') for log in logs)
          type_count = Counter(log.get('type', 'unknown') for log in logs)
          domain_count = Counter(log.get('name', 'unknown') for log in logs)
          client_count = Counter(log.get('clientIp', 'unknown') for log in logs)
          processed_data['statistics'] = {
              'by_status': dict(status_count),
              'by_type': dict(type_count),
              'by_client': dict(client_count.most_common(10)),
              'blocked_percentage': round((status_count.get('blocked', 0) / len(logs)) * 100, 2) if logs else 0
          }
          processed_data['top_domains'] = {
              'all': dict(domain_count.most_common(20)),
              'blocked': dict(Counter(
                  log.get('name') for log in logs
                  if log.get('status') == 'blocked'
              ).most_common(10))
          }
          os.makedirs('content/data', exist_ok=True)
          with open('content/data/processed_logs.json', 'w') as f:
              json.dump(processed_data, f, indent=2)
          summary_data = {
              'last_update': datetime.now().isoformat(),
              'total_queries': len(logs),
              'blocked_queries': status_count.get('blocked', 0),
              'allowed_queries': status_count.get('allowed', 0),
              'blocked_percentage': processed_data['statistics']['blocked_percentage']
          }
          with open('content/data/summary.json', 'w') as f:
              json.dump(summary_data, f, indent=2)
          print("‚úÖ Log processing completed")
          print(f"total_queries={len(logs)}")
          print(f"blocked_queries={status_count.get('blocked', 0)}")
          print(f"blocked_percentage={processed_data['statistics']['blocked_percentage']}")
          with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
            print(f"total_queries={len(logs)}", file=fh)
            print(f"blocked_queries={status_count.get('blocked', 0)}", file=fh)
            print(f"blocked_percentage={processed_data['statistics']['blocked_percentage']}", file=fh)
          EOF

      - name: Prepare Hugo Site and Update Theme
        if: steps.fetch-logs.outputs.success == 'true' || github.event.inputs.force_rebuild == 'true'
        run: |
          echo "üèóÔ∏è Preparing Hugo site structure..."
          if [ ! -f "config.toml" ]; then
            cat << 'EOF' > config.toml
baseURL = "https://diogo0587.github.io/lognextdns/painel/"
languageCode = "pt-br"
title = "NextDNS Logs Dashboard"
theme = "beautifulhugo"
[params]
  description = "Dashboard de logs do NextDNS"
  github_url = "https://github.com/diogo0587/lognextdns"
[markup]
  [markup.goldmark]
    [markup.goldmark.renderer]
      unsafe = false
EOF
          fi
          if [ ! -d "themes/beautifulhugo/.git" ]; then
            echo "Updating theme"
            rm -rf themes/beautifulhugo
            git clone https://github.com/halogenica/beautifulhugo.git themes/beautifulhugo
          else
            echo "Theme already up to date"
          fi
          mkdir -p content/{posts,pages} layouts/{_default,partials} static/{css,js}
          cat << 'EOF' > content/_index.md
---
title: "NextDNS Logs Dashboard"
description: "Dashboard de monitoramento dos logs do NextDNS"
---
# Dashboard NextDNS
Este dashboard apresenta informa√ß√µes em tempo real dos logs do NextDNS.
## Estat√≠sticas Recentes
Os dados s√£o atualizados automaticamente via GitHub Actions.
EOF

      - name: Generate Hugo Content from Logs
        if: steps.fetch-logs.outputs.success == 'true'
        run: |
          python3 << 'EOF'
          import json
          from datetime import datetime
          import os
          try:
              with open('content/data/processed_logs.json', 'r') as f:
                  data = json.load(f)
          except FileNotFoundError:
              print("‚ùå No processed data found")
              exit(1)
          stats = data['statistics']
          domains = data['top_domains']
          stats_content = f"""---
title: "Estat√≠sticas de Logs"
date: {datetime.now().isoformat()}
draft: false
---
# Estat√≠sticas NextDNS
**√öltima atualiza√ß√£o:** {data['metadata']['timestamp']}
## Resumo Geral
- **Total de consultas:** {data['metadata']['total_entries']:,}
- **Consultas bloqueadas:** {stats['by_status'].get('blocked', 0):,}
- **Consultas permitidas:** {stats['by_status'].get('allowed', 0):,}
- **Taxa de bloqueio:** {stats['blocked_percentage']}%
## Top 10 Dom√≠nios Mais Acessados
"""
          for i, (domain, count) in enumerate(domains['all'].items(), 1):
              if i <= 10:
                  stats_content += f"{i}. **{domain}** - {count:,} consultas\n"
          stats_content += "\n## Top 10 Dom√≠nios Bloqueados\n\n"
          for i, (domain, count) in enumerate(domains['blocked'].items(), 1):
              if i <= 10:
                  stats_content += f"{i}. **{domain}** - {count:,} bloqueios\n"
          with open('content/statistics.md', 'w') as f:
              f.write(stats_content)
          print("‚úÖ Hugo content generated")
          EOF

      - name: Build Hugo Site
        id: build-hugo
        if: steps.fetch-logs.outputs.success == 'true' || github.event.inputs.force_rebuild == 'true'
        run: |
          echo "üî® Building Hugo site..."
          hugo --minify --cleanDestinationDir --destination public/painel
          if [ -d "public/painel" ] && [ "$(ls -A public/painel)" ]; then
            echo "‚úÖ Public/painel directory contains files"
            ls -la public/painel/
            echo "success=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Public/painel directory is empty or missing"
            echo "success=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Test Hugo Site
        if: steps.build-hugo.outputs.success == 'true'
        run: |
          echo "üß™ Testing Hugo site..."
          # Add your tests here. For example:
          # - Check if the statistics page exists
          # - Check if the summary.json file exists
          # - Check if the index page exists
          if [ -f "public/painel/index.html" ] && [ -f "public/painel/statistics/index.html" ]; then
            echo "‚úÖ Basic tests passed"
          else
            echo "‚ùå Basic tests failed"
            exit 1
          fi

      - name: Send to Datadog (Optional)
        if: |
          steps.fetch-logs.outputs.success == 'true' &&
          github.event.inputs.skip_integrations != 'true'
        env:
          DATADOG_API_KEY: ${{ secrets.DATADOG_API_KEY }}
        continue-on-error: true
        id: datadog
        run: |
          if [ -z "${DATADOG_API_KEY}" ]; then
            echo "Datadog API key not set, skipping..."
            exit 0
          fi
          if [ -f "content/data/summary.json" ]; then
            python3 << 'EOF'
          import json
          import requests
          import os
          from datetime import datetime
          with open('content/data/summary.json', 'r') as f:
              data = json.load(f)
          datadog_api_key = os.environ.get('DATADOG_API_KEY')
          payload = {
              "series": [
                  {
                      "metric": "nextdns.total_queries",
                      "points": [[int(datetime.now().timestamp()), data['total_queries']]],
                      "tags": ["source:nextdns", "profile:85d564"]
                  },
                  {
                      "metric": "nextdns.blocked_queries",
                      "points": [[int(datetime.now().timestamp()), data['blocked_queries']]],
                      "tags": ["source:nextdns", "profile:85d564"]
                  }
              ]
          }
          try:
              response = requests.post(
                  "https://api.datadoghq.com/api/v1/series",
                  headers={"DD-API-KEY": datadog_api_key, "Content-Type": "application/json"},
                  json=payload,
                  timeout=30
              )
              response.raise_for_status()
              print("‚úÖ Metrics sent to Datadog")
              print("datadog_success=true")
          except Exception as e:
              print(f"‚ùå Failed to send to Datadog: {e}")
              print("datadog_success=false")
          with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
            print("datadog_success=true", file=fh)
          EOF
          fi
          if [ -z "${{ steps.datadog.outputs.datadog_success }}" ]; then
            echo "datadog_success=false" >> $GITHUB_OUTPUT
          fi

      - name: Commit Changes
        if: steps.fetch-logs.outputs.success == 'true'
        run: |
          git config --local user.name "github-actions[bot]"
          git config --local user.email "41898282+github-actions[bot]@users.noreply.github.com"
          git add .
          if git diff --staged --quiet; then
            echo "‚ÑπÔ∏è  No changes to commit"
          else
            TOTAL_QUERIES=$(jq .total_queries content/data/summary.json)
            BLOCKED_PERCENTAGE=$(jq .blocked_percentage content/data/summary.json)
            git commit -m "üìä Update NextDNS logs and dashboard - $(date +'%Y-%m-%d %H:%M')" \
                       -m "- Fetched and processed latest NextDNS logs" \
                       -m "- Updated Hugo dashboard" \
                       -m "- Total queries: ${TOTAL_QUERIES}" \
                       -m "- Blocked percentage: ${BLOCKED_PERCENTAGE}%"
            git push
            echo "‚úÖ Changes committed and pushed"
          fi

      - name: Deploy to GitHub Pages
        if: steps.build-hugo.outputs.success == 'true'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: ./public/painel
          force_orphan: true
          commit_message: "üöÄ Deploy NextDNS dashboard - $(date +'%Y-%m-%d %H:%M')"

      - name: Cleanup Old Files
        run: |
          find data/logs -name "raw_*.json" -type f | sort -r | tail -n +6 | xargs rm -f || true
          echo "üßπ Cleanup completed"

      - name: Workflow Summary
        if: always()
        run: |
          echo "## üìä NextDNS Unified Workflow Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Date:** $(date)" >> $GITHUB_STEP_SUMMARY
          echo "**Logs Fetched:** ${{ steps.fetch-logs.outputs.success }}" >> $GITHUB_STEP_SUMMARY
          echo "**Hugo Built:** ${{ steps.build-hugo.outputs.success }}" >> $GITHUB_STEP_SUMMARY
          echo "**Datadog Success:** ${{ steps.datadog.outputs.datadog_success }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          if [ "${{ steps.fetch-logs.outputs.success }}" == "true" ]; then
            if [ -f "content/data/summary.json" ]; then
              echo "### üìà Statistics" >> $GITHUB_STEP_SUMMARY
              TOTAL_QUERIES=$(jq .total_queries content/data/summary.json)
              BLOCKED_QUERIES=$(jq .blocked_queries content/data/summary.json)
              BLOCKED_PERCENTAGE=$(jq .blocked_percentage content/data/summary.json)
              echo "- Total Queries: ${TOTAL_QUERIES}" >> $GITHUB_STEP_SUMMARY
              echo "- Blocked: ${BLOCKED_QUERIES}" >> $GITHUB_STEP_SUMMARY
              echo "- Block Rate: ${BLOCKED_PERCENTAGE}%" >> $GITHUB_STEP_SUMMARY
            fi
          else
            echo "‚ùå Log fetching failed - check NextDNS API key and connectivity" >> $GITHUB_STEP_SUMMARY
          fi
